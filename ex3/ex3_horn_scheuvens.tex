\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}

\usepackage{amssymb, amsmath, amsthm, amsfonts, algorithmic, algorithm, graphicx}
\usepackage{color}
\usepackage{bbm}
\usepackage[dvipsnames]{xcolor} 
\usepackage[colorlinks,linkcolor=blue,citecolor=blue]{hyperref}
\usepackage{array}
\usepackage{ifthen}
\usepackage{mathtools}

\renewcommand{\baselinestretch}{1.1}
\setlength{\topmargin}{-3pc}
\setlength{\textheight}{8.5in}
\setlength{\oddsidemargin}{0pc}
\setlength{\evensidemargin}{0pc}
\setlength{\textwidth}{6.5in}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{question}[theorem]{Question}
\newtheorem{result}[theorem]{Result}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{assumption}[theorem]{Assumption}
\numberwithin{equation}{section}

\def \endprf{\hfill {\vrule height6pt width6pt depth0pt}\medskip}
\renewenvironment{proof}{\noindent {\bf Proof} }{\endprf\par}

% Notational convenience,
% real numbers 
\newcommand{\R}{\mathbb{R}}  
% Expectation operator
\DeclareMathOperator*{\E}{\mathbb{E}}
% Probability operator
\DeclareMathOperator*{\Prob}{\mathbb{P}}
\renewcommand{\Pr}{\Prob}

% You may define additional macros here.
\newcommand{\unita}{\begin{pmatrix} 1\\0\end{pmatrix}}
\newcommand{\unitb}{\begin{pmatrix} 0\\1\end{pmatrix}}
\newcommand{\gramint}{\int_{-1}^1}


\begin{document}

\begin{center}
    \sc ML 4101: Mathematics for Machine Learning --- Fall 24
\end{center}

\noindent Friederike Horn \& Bileam Scheuvens

Justify all your claims.
\section*{Exercise 1 (Symmetric Matrices)}
\begin{enumerate}
\item[a)]{
    Any orthogonal symmetric matrix $A \in \mathbb{R}^{n\times n}$ can only have the eigenvalues $-1$ or $1$.
  }
\item[b)]{
    Consider a symmetric matrix $A \in \mathbb{R}^{n\times n}$ and an orthogonal matrix $Q \in \mathbb{R}^{n\times n}$ and let $\tilde{A} = QAQ^T$. Then $\tilde{A}$ is again symmetric and has the same eigenvalues as $A$.
  }
\item[c)]{
    For any matrix $A \in \mathbb{R}^{n\times n}$ with $A^TA = I_m$ the matrix $AA^T$ is the orthogonal projection onto the space $range(A)$.
  }
\end{enumerate}
\subsection*{Solution}
\begin{enumerate}
\item[a)]{
    Since $A$ is symmetric $A = A^T$ and orthogonal $A^T A = I$ it follows that for any eigenvector $v$ with eigenvalue $\lambda$: $A^T Av = A^2v = Iv = \lambda^2v = v$. Therefore $\lambda$ must be $\sqrt{1}$.
  }
\item[b)]{
Let $A$ be a symmetric matrix and $Q$ an orthogonal matrix. \\ If we define $\tilde{A} = QAQ^T$ then the transpose is given by $\tilde{A}^T = (QAQ^T)^T = Q(Q A)^T = Q A^TQ^T= Q A Q^T = \tilde{A}$. Here we used that $(AB)^T = B^T A^T$ and $(A^T)^T=A$ for the first and second equality and the fact that $A$ is symmetric for the third equality. \\
Let $v_i$ be an eigenvector of $A$ with eigenvalue $\lambda_i$. If apply the vector $Q v_i$ to $\tilde{A}$ we obtain:\\
 $\tilde{A}Q v_i  = Q A Q^T Q v_i = Q A v_i = \lambda_i Q v_i$. \\
 Here we used the orthogonality of $Q$ in the second equation. It follows directly, that if $v_i$ is eigenvector of $A$ with eigenvalue $\lambda_i$, then $Q v_i$ is an eigenvector of $\tilde{A}$ with the same eigenvalue and $A$ and $\tilde{A}$ have the same eigenvalues. \\
  }
\item[c)]{
 We  $AA^T$ is the orthogonal projection onto range($A$) iff $ker(AA^T) = \text{range}(A)^\perp$. We first note that the zero vector is in range($A$) and $ker(AA^T)$.\\
  $\Rightarrow$ Let $v$ be a non-zero vector in $ker(AA^T)$, i.e. $AA^Tv= 0$. Then also $\left < Aw, AA^Tv\right > = 0$ for any vector $w$ in $ \mathbb{R}^{m}$. Writing out the inner product we have: $\left < Aw, AA^Tv\right > = w^T A ^T A A^Tv = w^T I_m A ^T v = \left<Aw, v\right >=0$, for any vector $w$. Therefore, $v$ is in range(A)$^\perp$. \\
  $\Leftarrow$ Let $v$ be a non-zero vector in range(A)$^\perp$, i.e. $\left< Aw, v\right> = 0$ for any $w \in \mathbb{R}^m$. Writing it out the inner product we have $\left< Aw, v\right> = w^T A^T v = w^T v' = 0$ for any $w \in \mathbb{R}^m$. This means that $v'=A^Tv = 0$ and therefore also $AA^Tv =0$ and $v$ in ker($AA^Tv$).
  }
\end{enumerate}

\section*{Exercise 2 (Scalar product and norms)}
Consider a normed vector space $(V, ||\cdot||)$. The parallelogram equality states that the norm is induced by a scalar product $\langle \cdot, \cdot \rangle: V \times V \rightarrow \mathbb{R}$ via $||v|| = \sqrt{\langle v, v \rangle}$ if and only if $\forall v, w \in V$ it holds that
$$ || v + w||^2 + ||v - w||^2 = 2(||v||^2 + ||w||^2)$$

\begin{enumerate}
\item[a)]{
    Prove the forward implication.
  }
\item[b)]{
    Let $p > 0$. Prove that there is a sclaar product on $\mathbb{R}^2$ such that the associated norm is given by 
$$||x|| = (|x_1|^p + |x_2|^p)^{1/p}$$
$\forall x \in \mathbb{R}^2$ iff $p =2$.

  }
\item[c)]{
    Let $(V, ||\cdot||)$ be a normed vector space, where the norm is given by a scalar product. Prove the Pythagorean theorem $||u + v||^2 = ||u||^2 + ||v||^2$ for all orthogonal $u,v \in V$

  }
\end{enumerate}
\subsection*{Solution}
\begin{enumerate}
\item[a)]{
$$\langle v, w \rangle = \sum_{i=1}^n v_i w_i$$
    \begin{align*}
      ||v + w||^2 ||v - w||^2 &= \left(\sqrt{\sum_{i=1}^n (v_i + w_i)^2}\right)^2 + \left(\sqrt{\sum_{i=1}^n (v_i - w_i)^2}\right)^2\\
      &= \sum_{i=1}^n (v_i + w_i)^2+ \sum_{i=1}^n (v_i - w_i)^2\\
      &= \sum_{i=1}^n v_i^2 +2v_i w_i + w_v^2 + \sum_{i=1}^n v_i^2 -2v_i w_i  + w_i^2\\
      &= \sum_{i=1}^n 2v_i^2 + 2w_v^2 \\
      &= 2 (\langle v,v \rangle \langle w,w \rangle)\\
      &= 2 (||v||^2 + ||w||^2)
    \end{align*}
  }
\item[b)]{
    Let $v = \unita, w = \unitb$
    \begin{align*}
      || v+w||^2  + || v-w||^2 &= 2(||v||^2 +||w||^2)\\
      || \begin{pmatrix}1\\1\end{pmatrix}||^2  + || \begin{pmatrix}1\\-1\end{pmatrix}||^2 &= 2\left(||\unita||^2 +||\unitb||^2\right)\\
      ((2)^{1/p})^2 + ((2)^{1/p})^2 &= 2\left( ((1^p)^{1/p})^2 + ((1^p)^{1/p})^2\right)\\
      2((2)^{1/p})^2 &= 2( 1^2 + 1^2)\\
      ((2)^{1/p})^2 &= 2\\
      \sqrt[p]{2} &= \sqrt[2]{2}\\
    \end{align*}
  }
\item[c)]{
Let $(V,\|\cdot\|)$ be a normed vector space, where the norm is given by a scalar product and $u, v \in V$ orthogonal vectors. Then we can write:
\begin{align*}
  \|u+v\|^2 &= \langle u+v, u+v\rangle\\
            &= \langle v, v\rangle + \langle u, u\rangle + \langle v, u\rangle + \langle u, v\rangle\\
            &=  \langle v, v\rangle + \langle u, u\rangle\\ 
            &= \|u\|^2+\|v\|^2\\
\end{align*}

where we used the bilinearity of the scalar product and the fact that $u, v$ are orthogonal and thus $ \langle v, u\rangle = \langle u, v\rangle=0.$
}
\end{enumerate}
\section*{Exercise 3 (Gram-Schmidt orthonormalization)}
\begin{enumerate}
\item[a)]{
    Consider the subspace $V = span(1, x, x^2 , x^3) \subseteq \mathbb{R}^{\mathbb{R}}$ of polynomials with degree $\leq 3$ with the scalar product $\langle f, g\rangle = \int_{-1}^1 f(x)g(x)dx$ for $f, g \in V$. Apply Gram-Schmidt to the basis $1, x, x^2 , x^3$ in order to compute an orthonormal basis of $V$.
  }
\item[b)]{
    What happens if Gram-Schmidt is applied to a list of vectors $(v_1, \dots, v_n)$ that is not linearly independent?
  }
\end{enumerate}

\subsection*{Solution}
\begin{enumerate}
\item[a)]{
    \begin{align*}
      u_1 &= 1\\
      e_1 &= \frac{u_1}{||u_1||} = \frac{1}{\sqrt{2}}\\
      u_2 &= v_2 - \text{proj}_{u_1} (v_2) = x - \frac{\langle v_2, u_1\rangle}{\langle u_1, u_1\rangle}\\
          &= x - \frac{\gramint x*1dx}{\gramint 1*1dx} = x - \frac{0}{2} = x\\
      e_2 &= \frac{u_2}{||u_2||} = \sqrt{\frac{3}{2}}x\\
      u_3 &= v_3 - \frac{\gramint x*x^2}{\gramint x*x} = x^2 - \frac{0}{2/3} = x^2\\
      e_3 &= \frac{u_3}{||u_3||} = \sqrt{\frac{5}{2}}x^2\\
      u_4 &= v_4 - \frac{\gramint x^2*x^3}{\gramint x^2*x^2} = x^3 - \frac{0}{2/5} = x^3\\
      e_4 &= \frac{u_4}{||u_4||} = \sqrt{\frac{7}{2}}x^3\\
    \end{align*}
    $$||e_1|| = ||e_2|| = ||e_3|| = ||e_4|| = 1$$
  }
\item[b)]{
    Then the projection of the first non independent vector onto the other vectors will be equal to the vector itself. Subtracting this projection from the vector yields the zero vector and the normalization step therefore divides by 0.
  }
\end{enumerate}

\section*{Exercise 4 (Spectral clustering)}
Let $W \in \mathbb{R}^{n\times n}$ be a symmetric matrix with non-negative entries and $D \in \mathbb{R}^{n\times n}$ the diagonal matrix which contains the row sums of $W$. Define $L := D - W$.


\begin{enumerate}
\item[a)]{
    Prove that for all $x \in \mathbb{R}^n$ it holds
    $$ x^t Lx = \frac{1}{2} \sum_{i,j=1}^n w_{i,j} (x_i - x_j)^2$$
  }
\item[b)]{
    Conclude that $L$ is symmetric and positive semi-definite.
  }
\item[c)]{
    Show that the vector of constant ones $\mathbbm{1} = \begin{pmatrix} 1 \\ \vdots \\ 1\end{pmatrix} \in \mathbb{R}^n$ is an eigenvector of $L$.
  }
\item[d)]{
  Solve the constrained minimization problem
  $$\underset{||x|| = 1}{\underset{x\in \mathbb{R}^n}{min}} x^t Lx \text{ subject to }\langle x, \mathbbm{1} \rangle = 0$$
}
\end{enumerate}

\subsection*{Solution}
\begin{enumerate}
  \item[a)]{
\begin{align*}
	x^T Lx &= \sum_{ij} x_i L_{ij} x_j = \sum_{ij} x_i\left(\sum_{k'}w_{ik'} \delta_{ij} x_j\right) - \sum_{ij} w_{ij}x_i x_j \\
	&= \sum_{i} x_i\left(\sum_{k'}w_{ik'} x_i\right) - \sum_{ij} w_{ij}x_i x_j \\
  &= \sum_{i, j} w_{ij} x_i^2 - \sum_{ij} w_{ij}x_i x_j  \\
	&= \frac{1}{2} \left( 2 \sum_{i, j} w_{ij} x_i^2 - 2\sum_{ij} w_{ij}x_i x_j\right) \\
	&= \frac{1}{2} \left(  \sum_{i, j} w_{ij} x_i^2 + \sum_{i, j} w_{ji} x_j^2 - 2\sum_{ij}w_{ij}x_i x_j\right) \\
\text{because of the symmetry of W} w_{ij} = w{ji}:\\
	&= \frac{1}{2} \left(  \sum_{i, j} w_{ij} x_i^2 + \sum_{i, j} w_{ij} x_j^2 - 2\sum_{ij}w_{ij}x_i x_j\right)\\
	&= \frac{1}{2} \left(  \sum_{i, j} w_{ij} (x_i^2 +  x_j^2 - 2 x_i x_j)\right)\\
	&=\frac{1}{2} \left(  \sum_{i, j} w_{ij} (x_i-x_j)^2\right)
\end{align*} 
    }
  \item[b)]{
      Since both $D$ and $W$ are symmetric any element $l_{i,j} = d_{i,j} + w_{i,j} = d_{j,i} + w_{j,i} = l_{j,i}$.\\ $L$ is positive semi-definite if $x^t Lx \geq 0 \; \forall x$. Since $W$ is non-negative and $(x_i - x_j)^2 \geq 0$, $\sum_{i,j=1}^n w_{i,j} (x_i - x_j)^2 \geq 0$ is satisfied and $L$ is positive semi-definite.
    }
  \item[c)]{
      The i-th entry of $L \mathbbm{1}$ is given by
      $$d_{i,i} - \sum_{j=1}^n w_{i,j}$$
      $$= \sum_{j=1}^n w_{i,j}- \sum_{j=1}^n w_{i,j}$$
      $$=0$$
      Therefore $\mathbbm{1}$ is an eigenvector with eigenvalue $0$.
    }
  \item[d)]{
      Consider ordering the eigenvalues $\lambda_1 < \lambda_2 < \dots < \lambda_n$ with $\lambda_1 \geq 0$ from positive semi-definiteness. From c) we know that $\lambda_1 = 0$. Using Rayleigh coefficient with additional constraint of orthogonality to $\mathbbm{1}$ (the eigenvector to $\lambda_1$) we obtain a minimum of $\lambda_2$ at the normed second eigenvector $\frac{v_2}{||v_2||}$.
    }
\end{enumerate}

\end{document}
