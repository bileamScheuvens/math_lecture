\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}

\usepackage{amssymb, amsmath, amsthm, amsfonts, algorithmic, algorithm, graphicx}
\usepackage{color}
\usepackage{bbm}
\usepackage[dvipsnames]{xcolor} 
\usepackage[colorlinks,linkcolor=blue,citecolor=blue]{hyperref}
\usepackage{array}
\usepackage{ifthen}
\usepackage{mathtools}

\renewcommand{\baselinestretch}{1.1}
\setlength{\topmargin}{-3pc}
\setlength{\textheight}{8.5in}
\setlength{\oddsidemargin}{0pc}
\setlength{\evensidemargin}{0pc}
\setlength{\textwidth}{6.5in}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{question}[theorem]{Question}
\newtheorem{result}[theorem]{Result}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{assumption}[theorem]{Assumption}
\numberwithin{equation}{section}

\def \endprf{\hfill {\vrule height6pt width6pt depth0pt}\medskip}
\renewenvironment{proof}{\noindent {\bf Proof} }{\endprf\par}

% Notational convenience,
% real numbers 
\newcommand{\R}{\mathbb{R}}  
% Expectation operator
\DeclareMathOperator*{\E}{\mathbb{E}}
% Probability operator
\DeclareMathOperator*{\Prob}{\mathbb{P}}
\renewcommand{\Pr}{\Prob}

% You may define additional macros here.
\newcommand{\unita}{\begin{pmatrix} 1\\0\end{pmatrix}}
\newcommand{\unitb}{\begin{pmatrix} 0\\1\end{pmatrix}}
\newcommand{\gramint}{\int_{-1}^1}
\newcommand{\limn}{\underset{n\rightarrow \infty}{lim}}
\newcommand{\epix}{e^{\pi-x}}


\begin{document}

\begin{center}
    \sc ML 4101: Mathematics for Machine Learning --- Fall 24
\end{center}

\noindent Friederike Horn \& Bileam Scheuvens

Justify all your claims.
\section*{Exercise 1 (Extremal points)}
Consider the function $f: \mathbb{R}^2 \to \mathbb{R}; (x, y) \mapsto x^3 + 1/3y^3 -12x -y$
\subsection*{Solution}
\begin{enumerate}
\item[a)]{
$(x, y)$ is an extremal point if $\frac{\partial f}{\partial x}=0$ and $\frac{\partial f}{\partial y} = 0$. \\
$$\frac{\partial f}{\partial x} = 3 x^2 -12$$
$$\frac{\partial f}{\partial y} = y^2 -1$$

From this we can get four different extremal points:
$$(x_1, y_1) = (2, 1)$$
$$(x_2, y_2) = (-2, 1) $$
$$(x_3, y_3) = (2, -1) $$
$$ (x_4, y_4) = (-2, -1) $$
In order to classify them we need to compute the Hessian: 
$$
H = \begin{pmatrix} 6x & 0 \\
0 & 2y \end{pmatrix}.
$$
We directly see that this is diagonal and thus the eigenvalues correspond to the diagonal entries \\
We see that  for $(x_1, y_1)$ we have positive eigenvalues and therefore a strict  local minimum. Likewise, for $(x_4, y_4)$ we have negative eigenvalues and therefore a strict local  maximum. For the other two points the Hessian matrix has a negative and a positive eigenvalue and thus it is indefinite and we have two saddle points.}
\item[b)] {
(x, y) is a global maximum iff there exists no other $(x', y')$ such that $f(x', y') > f(x, y))$ and likewise for global minimum. \\
In this case the function has no global maximum or minimum. E.g. $f(x_1, y_1) = 8 + 1/3 -24 -1 = -17.3$, but we find that $f(-3, 0) = -27 < -17.3$. \\
Similarly, $f(x_4, y_4) = -8 - 1/3 + 24 +1 = 16.67$, but $f(3, 0)= 27 > 16.67 = f(x_4, y_4)$. 
}
\item[c)] {
Consider $g: \mathbb{R}^3 \to \mathbb{R}, (x, y, z) \mapsto \alpha x^2e^y + y^2 e^z + z^2 e^x$, with $\alpha \in \mathbb{R}$. 
The point $(0, 0, 0)$ is an extremal point (local, minimum, maximum or saddle point) iff the first derivative is zero i.e. $\nabla f = \textbf{0}$.
$$
\nabla f = \begin{pmatrix} \alpha 2x e^y + z^2 e^x\\ \alpha x^2 e^y + 2y e^z \\ y^2 e^z + 2z e^x \end{pmatrix}_{(0, 0)}
$$
$$
= \begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}
$$
To differentiate between the different points we compute the Hessian:
$$
H = {\begin{pmatrix}
\alpha 2 e^y + z^2e^x & \alpha 2x e^y & 2z e^x \\
\alpha 2x e^y  & x^2 e^y + 2 e^z & 2ye^z \\
2z e^x & 2y e^z & y^2 e^z + 2 e^x
\end{pmatrix}}_{(0, 0)}
$$
$$
\begin{pmatrix}
\alpha 2   &  0& 0  \\
0  & 2 & 0 \\
0 & 0 &  2 
\end{pmatrix}.
$$
As this is again a diagonal matrix we can directly read out the eigenvalues from the diagonal entries and as two af them are greater than zero we can only have a local minimum or saddle point. If $\alpha <0$ the matrix is indefinite and we therefore have a saddle point. If $a\geq 0$ we have a local minimum. 
}
\end{enumerate}
\section*{Exercise 2 (Derivatives)}

\begin{enumerate}
\item[a)]{
The partial derivative with respect to $x$ exists iff the limit $\lim_{\epsilon \to 0} \frac{f(x, y) - f(x+\epsilon, y)}{\epsilon}$ exists (analogous  for derivative to $y$).  
To compute the derivative for points $(x, y) \neq (0, 0)$ we first compute:
$$
\frac{\partial \left(x^2 + y^2\right)^{-1/2}}{\partial x}  = -\frac{1}{2} \frac{2x}{\left(x^2 + y^2\right)^{3/2}} = -\frac{x}{\left(x^2 + y^2\right)^{3/2}}
$$
We rename $x_2 = y$ and $\sqrt{x_1^2 + x_2^2} = r$ we can compute the partial derivative: 
$$
\frac{\partial f}{\partial x} = -\frac{x^2y}{r^3}sin(1/r) + \frac{y}{r}sin(1/r) -\frac{x^2y}{r^4} cos(1/r)
$$
$$
\frac{y^3}{r^3} sin(1/r) -\frac{x^2y}{r^4} cos(1/r)
$$
From this we see that the derivative exists for all non-zero points. \\
At zero we can write the limit:
$$
\lim_{\epsilon \to 0} \frac{f(\epsilon, 0) - f(0, 0)} {\epsilon} = 0 - 0 = 0.
$$
This means the limit is defined and the partial derivative exists for all $(x, y)$. 
As the function is symmetric under exchange of $x \to y$ the same is true for the partial derivative with respect to $y$.\\
The total derivative exists iff the partial derivative is continuous in all points. Again, the only point we have to check is zero, i.e. we have to check that:
$lim_{(x, y) \to (0, 0)} \frac{\partial f}{\partial x} = \frac{\partial f}{\partial x} (0, 0)$ (and likewise for $y$).
We approach the point $(0, 0)$ from $(0, \epsilon)$. 
$$
\lim_{\epsilon \to 0} \frac{\partial f}{\partial x}( 0, \epsilon) = \lim_{\epsilon \to 0} \sin(1/\epsilon^2) - 0.
$$
This limit does not exist, the partial derivative is not continuous and therefore the total derivative does not exist.
Since both partial derivatives exist, all directional derivatives exist but are also discontinuous at (0,0).}
\item[b)]{
For all non-zero points we directly find the partial derivatives with:
$$
\frac{\partial g}{ \partial x_1} = \frac{3x_1^2x_2}{x_1^4+x_2^2} - \frac{4x_1^6x_2}{(x_1^4 + x^2)^2}
$$
$$
\frac{\partial g}{ \partial x_2} = \frac{x_1^3}{x_1^4+{x_2}^2} - \frac{2x_1^3x_2^2}{(x_1^4 + x^2)^2}. 
$$
For $(x, y)= (0, 0)$ we find the partial derivative by: 
$$
\lim_{\epsilon \to 0} \frac{g(\epsilon, 0) - f(0, 0)} {\epsilon} = 0 - 0 = 0
$$
and analogously for the limit with respect to small variations in $y$:
$$
\lim_{\epsilon \to 0} \frac{g(0, \epsilon) - g(0, 0)} {\epsilon} = 0 - 0 = 0
$$
So both partial derivatives, and therefore all directional derivatives exist over the whole domain.
However, the partial derivatives are again not continous in $(0, 0)$. 

If we take the limit:
$$
\lim_{\epsilon \to 0} \frac{\partial g(\epsilon, \epsilon^2)}{\partial x} = \frac{\epsilon^5}{2\epsilon^4} - 4 \frac{\epsilon^8}{4 \epsilon^8} = 0 - 1 = -1 \neq \frac{\partial g}{\partial x} (0, 0) = 0.
$$
Therefore, the partial derivative with respect to $x$ is not continuous and thus the total derivative does not exist. 
}

\end{enumerate}

\section*{Exercise 3 (Taylor Series)}
Given the function $f: \mathbb{R}^2 \to \mathbb{R}; (x_1, x_2) \mapsto \frac{1}{1-x_1-x_2}$ find the Taylor series around point $(0, 0)$ and the set over which it converges. 
\subsection*{Solution}
We first note for the derivatives:
$$
\frac{\partial f^n}{{\partial x_1}^\alpha{\partial x_2}^{n-\alpha}} = n! \frac{1}{{(1 - x_1 - x_2)}^n}.  
$$
This is because the partial derivative in $x_1$ direction is equal to the derivative in $x_2$ direction and each derivative simply multiplies by the power of the denominator and then increases the power of the denominator by one. \\
For this reason the multivariate Taylor series (around the point $(0, 0)$) can immediately be written out as:
$$
T_f = \sum_{n=0}^\infty \sum_{\alpha=0}^n \frac{n!}{\alpha! (n-\alpha)!}x_1^\alpha x_2^{n-\alpha}.
$$
This can be rewritten as:
$$
T_f = \sum_{n=0}^\infty \sum_{\alpha=0}^n \binom{n}{\alpha} x_1^\alpha x_2^{n-\alpha} = \sum_{n=0}^\infty (x_1 + x_2)^n.
$$
This series is the geometric series and converges iff $|x_1 + x_2| < 1$.  
\section*{Exercise 4 (Matrix Cookbook)}
\subsection*{Solution}
\begin{enumerate}
\item[a)]{
$$
\frac{\partial a^tx}{\partial x} = \begin{pmatrix} 
\frac{\partial a^tx}{\partial x_1} \\
\frac{\partial a^tx}{\partial x_2}\\
\frac{\partial a^tx}{\partial x_n}\end{pmatrix}
$$
With $a^tx = a_1 x_1 + a_2 x_2 + ... + a_n x_n$ we conclude:
$$
\frac{\partial a^tx}{\partial x} = \begin{pmatrix} 
a_1 \\
a_2 \\
a_n\end{pmatrix} = a.
$$\\
Secondly, $x^tAx = \sum_{i} x_i \sum_{j} A_ij x_j$ and thus the gradient is:
$$
\frac{\partial x^t Ax}{\partial x} = \begin{pmatrix} 
\frac{\partial x^tAx}{\partial x_1} \\
\frac{\partial x^tAx}{\partial x_2}\\
...\\
\frac{\partial x^tAx}{\partial x_n}\end{pmatrix}
$$
$$
=\begin{pmatrix} 
\sum_{j}A_{1j} x_j + \sum_{i}A_{i1} x_i \\
\sum_{j}A_{2j} x_j + \sum_{i}A_{i2} x_i\\
...\\
\sum_{j}A_{nj} x_j + \sum_{i}A_{in} x_i\end{pmatrix}
$$
$$
=\begin{pmatrix} 
\sum_{j}A_{1j} x_j + \sum_{j}A^T_{1j} x_j \\
\sum_{j}A_{2j} x_j + \sum_{j}A^T_{2j} x_j\\
...\\
\sum_{j}A_{nj} x_j + \sum_{j}A^T_{nj} x_j
\end{pmatrix}
$$
where we renamed the $i\to j$ and used that $A_{ij} = A^T_{ji}$. 
But this simply corresponds to $(A+A^t)x$. }
\item[b)]{
To minimize with respect to $w$ we first write out the norm squared:
$$
\frac{1}{n} ||Xw -Y||^2 + \lambda ||w||^2 = 
\frac{1}{n} (Xw -Y)^T(Xw -Y) + \lambda w^T w
$$
$$
=\frac{1}{n}( w^TX^TXw - w^TX^TY - Y^TXw + Y^TY) + \lambda w^T w = f.
$$
We then take the derivative with respect to $w$ to find the extremal points. With the results from $(a)$ we obtain:
$$
\frac{\partial f}{\partial w}=\frac{1}{n}( (X^TX + {(X^TX)}^T) w - 2Y^TX) + \lambda w.
$$
Here we also used that we are in $\mathbb{R}$ and thus $w^T X^Ty = y^TXw$. 
Setting the derivative to zero we find:
$$
Y^TX = \left( X^TX + \frac{n}{2}\lambda Id \right) w 
$$
$$
\leftrightarrow w = \left( X^TX + \frac{n}{2}\lambda Id \right)^{-1} Y^TX
$$}
\item[c)] {
$$D \log(\det{X}) = \frac{1}{\det(X)} * D det(x)$$
We can write out the determinant of a matrix in Laplace expansion:
$$
\det(X)= \sum_{j=1}^n (-1)^{i+j} X_{ij} M_{ij}
$$
and therefore:
$$
\frac{\partial \det(X)}{\partial X_{ij}} = (-1)^{i+j} M_{ij}
$$
Using the definition of the inverse of a non-singular matrix in terms of $M_{ij}$: $A^{-1}_{ij}= \frac{1}{\det(X)} X^{-1}_{ij}$ we finally obtain:
$$
D \log(\det{X}) = \frac{1}{\det(X)} \cdot \det(X) X^{-1} = X^{-1} $$
}
\end{enumerate}
\end{document}
