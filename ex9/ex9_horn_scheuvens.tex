\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}

\usepackage{amssymb, amsmath, amsthm, amsfonts, algorithmic, algorithm, graphicx}
\usepackage{color}
\usepackage{bbm}
\usepackage[dvipsnames]{xcolor} 
\usepackage[colorlinks,linkcolor=blue,citecolor=blue]{hyperref}
\usepackage{array}
\usepackage{ifthen}
\usepackage{mathtools}
\usepackage[thinc]{esdiff}

\renewcommand{\baselinestretch}{1.1}
\setlength{\topmargin}{-3pc}
\setlength{\textheight}{8.5in}
\setlength{\oddsidemargin}{0pc}
\setlength{\evensidemargin}{0pc}
\setlength{\textwidth}{6.5in}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{question}[theorem]{Question}
\newtheorem{result}[theorem]{Result}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{assumption}[theorem]{Assumption}
\numberwithin{equation}{section}

\def \endprf{\hfill {\vrule height6pt width6pt depth0pt}\medskip}
\renewenvironment{proof}{\noindent {\bf Proof} }{\endprf\par}

% Notational convenience,
% real numbers 
\newcommand{\R}{\mathbb{R}}  
% Expectation operator
\DeclareMathOperator*{\E}{\mathbb{E}}
% Probability operator
\DeclareMathOperator*{\Prob}{\mathbb{P}}
\renewcommand{\Pr}{\Prob}

% You may define additional macros here.
\newcommand{\unita}{\begin{pmatrix} 1\\0\end{pmatrix}}
\newcommand{\unitb}{\begin{pmatrix} 0\\1\end{pmatrix}}
\newcommand{\gramint}{\int_{-1}^1}
\newcommand{\limn}{\underset{n\rightarrow \infty}{lim}}
\newcommand{\epix}{e^{\pi-x}}


\begin{document}

\begin{center}
    \sc ML 4101: Mathematics for Machine Learning --- Fall 24
\end{center}

\noindent Friederike Horn \& Bileam Scheuvens

\section*{$\sigma$-Algebra}

\subsection*{Solution}

Since $\mathcal{A}$ is a $\sigma$-Algebra, it must be closed under complement and under countable unions.
Using De Morgan's laws: $\overline{\underset{i \in I}\bigcup X_i} = \underset{i \in I}\bigcap \overline{X_i}$, we find that we can construct any intersection from unions and intersection, thus $\mathcal{A}$ is closed under intersection as well.
Given that $\mathcal{A}$ separates points, it must contain every set of a single element, as this separates the remaining point from all others. 
If all singletons are in $\mathcal{A}$ then all possible subsets of $X$ can trivially be constructed from the operations of countable intersection, union and complement without exiting $\mathcal{A}$. Thus $\mathcal{A} = \mathcal{P}(X)$.

\section*{Covariance and Correlation}

\subsection*{Solution}
\begin{enumerate}
\item[a)]{


}
\item[b)]{
    $$\varrho_{X,Y} = \frac{Cov(X,Y)}{\sigma_X \sigma_Y}$$
    Using the Cauchy-Schwarz Inequality:
  $$|Cov(X,Y)|^2 \leq \sigma_X^2 \sigma_Y^2$$
  $$\Rightarrow |Cov(X,Y)| \leq \sigma_X \sigma_Y$$
  Assuming $\sigma_X, \sigma_Y \neq 0$, divide by the product:
  $$\frac{|Cov(X,Y)|}{\sigma_X \sigma_Y} \leq 1$$
  $$ \Rightarrow -1\leq \frac{Cov(X,Y)}{\sigma_X \sigma_Y} \leq 1$$
$$ \Rightarrow -1\leq \varrho_{X,Y} \leq 1$$

}
\item[c)]{

  }

\end{enumerate}


\section*{Bernoulli Trials}
\subsection*{Solution}

\begin{enumerate}
\item[a)]{

}
\item[b)]{
    For a success on trial $t$ we need the first $t-1$ trials to be failures, followed by a success. Thus $P(T_1 = t) = (1-p)^{t-1}p$.\\
    For the second success to be on trial $t$, we need $t-2$ failures with a success in between at some point and at trial $t$. Since we have $t-1$ possible position for the first success, this leavus us with $P(T_2 = t) = (t-1)(1-p)^{t-1}p^2$.\\
    As $X = T_2 - T_1$ describes the number of trials between the first and second success, this is identical distributed as $T_1$, namely: $(1-p)^{t-1}p$.\\
}
\item[c)]{


}
\end{enumerate}

\section*{Independance}
\subsection*{Solution}
\begin{enumerate}
  \item[a)]{
      Assume X is independant of itself. Then 
      $$\Pr(X \leq c) \cap \Pr(X \leq c) = \Pr(X \leq c) * \Pr(X \leq c)$$
      $$ = \Pr(X \leq c)^2$$
      Additionally, regardless of whether $X \leq c$ it holds that:
      $$\Pr(X \leq c) \cap \Pr(X \leq c) = \Pr(X \leq c)$$
      Thus 
      $$\Pr(X \leq c) = \Pr(X \leq c)^2$$
      Which can only be the case if the $\Pr(X \leq c) \in \{0,1\}$.
      The same argument works $X < c$ to show that $\Pr(X < c) \in \{0,1\}$ too.
      Since:
      $$\Pr(X=c) =  \Pr(X \leq c)- \Pr(X < c)$$
      It follows that $\Pr(X=c) \in \{0,1\}$ for any c.
      Thus X is constant if it is independant of itself. \\
      It remains to show that X is independant if it is constant.
      Suppose $\Pr(X=c) =1$, then:
      $$\Pr(X=c) \cap \Pr(X=c) = 1 * 1 = \Pr(X=c) \Pr(X=c) $$
      Alternatively suppose $\Pr(X=c) = 0$
      $$\Pr(X=c) \cap \Pr(X=c) = 0 * 0 = \Pr(X=c) \Pr(X=c) $$
      Thus X is independant of itself if it is constant, concluding the proof.

    }
\end{enumerate}
\end{document}
